Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/02/23 17:33:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Loading data complete
Splitting data complete
Hypertuning 1 evaluations, on 6 cores:

  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]

Hyper Parameters:
  batch_size	  500
  dropout   	  0
  hidden_dim	  21
  learning_rate	  1e-12
  min_epochs	  50
  num_layers	  1
  output_dim	  1
  scaler_id 	  std
  svm_gamma 	  scale
  svm_nu    	  0.05
Device: cpu
Max number of batches: 130
Branch filler jit, done in: 0.4312171936035156
Dataprep, done in: 0.5211975574493408
frac diff: -0.0015140003026413965,  eps: 1e-08 
Algorithm failed: not done learning in max epochs.
frac diff: -0.0033289323577082998,  eps: 1e-08 
Algorithm failed: not done learning in max epochs.
frac diff: -0.0015589596982779513,  eps: 1e-08 
Algorithm failed: not done learning in max epochs.
frac diff: -0.0009450525757846059,  eps: 1e-08 
Algorithm failed: not done learning in max epochs.
Failed in: 00:19:19
                                                                                100%|██████████| 1/1 [19:27<00:00, 1167.17s/trial, best loss: 10.0]100%|██████████| 1/1 [19:27<00:00, 1167.17s/trial, best loss: 10.0]Total Trials: 1: 1 succeeded, 0 failed, 0 cancelled.


Hypertuning completed on dataset:
/data/alice/wesselr/JetToyHIResultSoftDropSkinny_500k.root

Best Hyper Parameters:
  batch_size	  500
  dropout   	  0
  hidden_dim	  21
  learning_rate	  1e-12
  min_epochs	  50
  num_layers	  1
  output_dim	  1
  scaler_id 	  std
  svm_gamma 	  scale
  svm_nu    	  0.05
with loss: 10.0
