{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using algorithm 1 of Unsupervised Anomaly Detection With LSTM Neural Networks\n",
    "Sauce: Tolga Ergen and Suleyman Serdar Kozat, Senior Member, IEEE\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "Algorithm 1: Quadratic Programming-Based Training for the Anomaly Detection Algorithm\n",
    "             Based on OC-SVM\n",
    "-----------------------------------------------------------------------------------------\n",
    "1. Initialize the LSTM parameters as θ_0 and the dual OC-SVM parameters as α_0\n",
    "2. Determine a threshold ϵ as convergence criterion\n",
    "3. k = −1\n",
    "4. do\n",
    "5.    k = k+1\n",
    "6.    Using θ_k, obtain {h}^n_{i=1} according to Fig. 2\n",
    "7.    Find optimal α_{k+1} for {h}^n_{i=1} using (20) and (21)\n",
    "8.    Based on α_{k+1}, obtain θ_{k+1} using (24) and Remark 3\n",
    "8. while (κ(θ_{k+1}, α{k+1})− κ(θ_k, α))^2 > ϵ\n",
    "9. Detect anomalies using (19) evaluated at θ_k and α_k\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "(20): α_1 = 1 − S − α_2, where S= sum^n_{i=3} α_i.\n",
    "(21): α_{k+1,2} = ((α_{k,1} + α_{k,2})(K_{11} − K_{12})  + M_1 − M_2) / (K_{11} + K_{22}\n",
    "                                                                               − 2K_{12})\n",
    "      K_{ij} =def= h ^T_iT h _j, Mi =def= sum^n_{j=3} α_{k,j}K_{ij}\n",
    "(24): W^(·)_{k+1} = (I + (mu/2)A_k)^-1 (I− (mu/2)A_k) W^(·)_k\n",
    "      Ak = Gk(W(·))T −W(·)GT\n",
    "\n",
    "Dual problem of the OC-SVM:\n",
    "(22): min_{theta}  κ(θ, α_{k+1}) = 1/2 sum^n_{i=1} sum^n_{j=1} α_{k+1,i} α_{k+1,j} h^T_i h_j\n",
    "(23): s.t.: W(·)^T W(·) = I, R(·)^T R(·) = I and b(·)^T b(·) = 1\n",
    "\n",
    "Remark 3: For R(·) and b(·), we first compute the gradient of the objective function with\n",
    "respect to the chosen parameter as in (25). We then obtain Ak according to the chosen\n",
    "parameter. Using Ak, we update the chosen parameter as in (24).\n",
    "\n",
    "         ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Using algorithm 2 of Unsupervised Anomaly Detection With LSTM Neural Networks\n",
    "Sauce: Tolga Ergen and Suleyman Serdar Kozat, Senior Member, IEEE]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#### imported from analysis/test_using_towardsdatascience_lstm.py ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# from sklearn.externals import joblib\n",
    "# import joblib\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from functions.data_saver import save_results, DataTrackerTrials\n",
    "from functions.data_manipulation import (\n",
    "    train_dev_test_split,\n",
    "    format_ak_to_list,\n",
    "    branch_filler,\n",
    "    lstm_data_prep,\n",
    "    get_full_pytorch_weight,\n",
    "    put_weight_in_pytorch_matrix,\n",
    ")\n",
    "from functions.data_loader import load_n_filter_data\n",
    "from functions.optimization_orthogonality_constraints import optimization\n",
    "\n",
    "from ai.model_lstm import LSTMModel\n",
    "\n",
    "from autograd import elementwise_grad as egrad\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "file_name = \"samples/JetToyHIResultSoftDropSkinny.root\"\n",
    "\n",
    "# Variables:\n",
    "batch_size = 210\n",
    "\n",
    "output_dim = 1\n",
    "layer_dim = 1\n",
    "dropout = 0.2\n",
    "n_epochs = 4\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "eps = 1e-2  # test value for convergence\n",
    "\n",
    "# Load and filter data for criteria eta and jetpt_cap\n",
    "_, _, g_recur_jets, _ = load_n_filter_data(file_name)\n",
    "g_recur_jets = format_ak_to_list(g_recur_jets)\n",
    "\n",
    "# only use g_recur_jets\n",
    "train_data, dev_data, test_data = train_dev_test_split(g_recur_jets, split=[0.8, 0.1])\n",
    "\n",
    "train_data, track_jets_train_data = branch_filler(train_data, batch_size=batch_size)\n",
    "dev_data, track_jets_dev_data = branch_filler(dev_data, batch_size=batch_size)\n",
    "\n",
    "# Only use train and dev data for now\n",
    "scaler = (\n",
    "    MinMaxScaler()\n",
    ")  # Note this has to be saved with the model, to ensure data has the same form.\n",
    "train_loader = lstm_data_prep(\n",
    "    data=train_data, scaler=scaler, batch_size=batch_size, fit_flag=True\n",
    ")\n",
    "val_loader = lstm_data_prep(data=dev_data, scaler=scaler, batch_size=batch_size)\n",
    "\n",
    "input_dim = len(train_data[0])\n",
    "hidden_dim = batch_size\n",
    "\n",
    "model_params = {\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"layer_dim\": layer_dim,\n",
    "    \"output_dim\": output_dim,\n",
    "    \"dropout_prob\": dropout,\n",
    "}\n",
    "\n",
    "# lstm model\n",
    "lstm_model = LSTMModel(**model_params)\n",
    "\n",
    "# svm model\n",
    "svm_model = OneClassSVM(nu=0.5, gamma=0.35, kernel=\"rbf\")\n",
    "\n",
    "# path for model - only used for saving\n",
    "# model_path = f'models/{lstm_model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def lstm_results(lstm_model, train_loader):\n",
    "    h_bar_list = []\n",
    "\n",
    "    i = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        jet_track_local = track_jets_train_data[i]\n",
    "        i += 1\n",
    "\n",
    "        x_batch = x_batch.view([batch_size, -1, model_params[\"input_dim\"]]).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        ### Train step\n",
    "        # set model to train\n",
    "        lstm_model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat, hn, theta = lstm_model(x_batch)\n",
    "\n",
    "        # get mean pooled hidden states\n",
    "        h_bar = hn[:, jet_track_local]\n",
    "\n",
    "        # h_bar_list.append(h_bar) # TODO, h_bar is not of fixed length! solution now: append all to list, then vstack the list to get 2 axis structure\n",
    "        h_bar_list.append(h_bar)\n",
    "\n",
    "        # also append w, r and b values\n",
    "\n",
    "        # a =[hn.T[x] for x in jet_track][0][i,:].cpu().detach().numpy() selects i-th \"mean pooled output\"\n",
    "        # a.dot(a) = h.T * h = scalar\n",
    "    return torch.vstack([h_bar[0] for h_bar in h_bar_list]), theta\n",
    "\n",
    "\n",
    "def lstm_results_np(lstm_model, train_loader):\n",
    "    h_bar_list, theta = lstm_results(lstm_model, train_loader)\n",
    "    return np.array([h_bar.detach().numpy() for h_bar in h_bar_list]), theta\n",
    "\n",
    "\n",
    "def kappa(alphas, a_idx, h_list):\n",
    "    out = 0\n",
    "    for idx1, i in enumerate(a_idx):\n",
    "        for idx2, j in enumerate(a_idx):\n",
    "            out += 0.5 * alphas[0, idx1] * alphas[0, idx2] * (h_list[i].T @ h_list[j])\n",
    "    return out\n",
    "\n",
    "\n",
    "def diff_argument(lstm_model, train_loader, alphas, a_idx, lr, W=None, R=None, b=None):\n",
    "\n",
    "    h_list, _ = lstm_results(lstm_model, train_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if W:\n",
    "            delta = W - lr * W\n",
    "            lstm_model.lstm.weight_ih_l0.copy_(delta)\n",
    "        elif R:\n",
    "            delta = R - lr * R\n",
    "            lstm_model.lstm.weight_hh_l0.copy_(delta)\n",
    "        elif b:\n",
    "            lstm_model.lstm.weight_ih_l0\n",
    "\n",
    "    h_list_new, _ = lstm_results(lstm_model, train_loader)\n",
    "\n",
    "    return (kappa(alphas, a_idx, h_list) - kappa(alphas, a_idx, h_list_new)) / delta\n",
    "\n",
    "\n",
    "def delta_func(\n",
    "    lstm_model,\n",
    "    train_loader,\n",
    "    h_list,\n",
    "    weight,\n",
    "    weight_name: str,\n",
    "    mu,\n",
    "    alphas,\n",
    "    a_idx,\n",
    "    pytorch_weights,\n",
    "):\n",
    "\n",
    "    delta = mu * weight\n",
    "    new_weight = weight - delta\n",
    "\n",
    "    # Use torh.no_grad to not record changes in this section\n",
    "    with torch.no_grad():\n",
    "\n",
    "        lstm_model_new = copy(\n",
    "            lstm_model  # Needs a copy, to avoid unexpected changes in the original model\n",
    "        )\n",
    "\n",
    "        # only updated desired weight element\n",
    "        pytorch_weights = put_weight_in_pytorch_matrix(\n",
    "            new_weight, weight_name, pytorch_weights\n",
    "        )\n",
    "\n",
    "        getattr(lstm_model_new.lstm, weight_name[5:]).copy_(pytorch_weights)\n",
    "\n",
    "    h_list_new, _ = lstm_results(lstm_model_new, train_loader)\n",
    "    return (\n",
    "        kappa(alphas, a_idx, h_list) - kappa(alphas, a_idx, h_list_new)\n",
    "    ) / delta  # Alphas, en a_idx worden niet correct geimporteerd in de functie\n",
    "\n",
    "\n",
    "def updating_theta(lstm_model, train_loader, h_list, theta: dict, mu, alphas, a_idx):\n",
    "\n",
    "    updated_theta = dict()\n",
    "\n",
    "    # Loop over all weight types (w,r,bi,bh)\n",
    "    for weight_type, weights in theta.items():\n",
    "\n",
    "        track_weights = dict()\n",
    "\n",
    "        # get full original weights, called pytorch_weights (following lstm structure)\n",
    "        pytorch_weights = get_full_pytorch_weight(weights)\n",
    "\n",
    "        for weight_name, weight in weights.items():\n",
    "            # follow stepts from eq. 24 in paper Tolga\n",
    "\n",
    "            pytorch_weights_layer = pytorch_weights[weight_name[-1]]\n",
    "\n",
    "            # derivative of function e.g. F = (25) from Tolga\n",
    "            g = delta_func(\n",
    "                lstm_model,\n",
    "                train_loader,\n",
    "                h_list,\n",
    "                weight,\n",
    "                weight_name,\n",
    "                mu,\n",
    "                alphas,\n",
    "                a_idx,\n",
    "                pytorch_weights_layer,\n",
    "            )\n",
    "\n",
    "            a = g @ weight.T - weight @ g.T\n",
    "            i = torch.eye(weight.shape[0])\n",
    "\n",
    "            # next point from Crank-Nicolson-like scheme\n",
    "            track_weights[weight_name] = (\n",
    "                torch.inverse(i + mu / 2 * a) @ (i - mu / 2 * a) @ weight\n",
    "            )\n",
    "\n",
    "        # store in theta\n",
    "        updated_theta[weight_type] = track_weights\n",
    "\n",
    "    return updated_theta\n",
    "\n",
    "\n",
    "def update_lstm(lstm, theta):\n",
    "    for weight_type, weights in theta.items():\n",
    "        # get full original weights, called pytorch_weights (following lstm structure)\n",
    "        pytorch_weights = get_full_pytorch_weight(weights)\n",
    "\n",
    "        weight_name = list(weights.keys())[0][5:-1]\n",
    "\n",
    "        for i in range(len(weights) // 4):\n",
    "            with torch.no_grad():\n",
    "                getattr(lstm, weight_name + str(i)).copy_(pytorch_weights)\n",
    "\n",
    "    return lstm\n",
    "\n",
    "\n",
    "def optimization(lstm, train_loader, alphas, a_idx, mu, h_list, theta):\n",
    "\n",
    "    # obtain W, R and b from current h\n",
    "    # W = h.parameters\n",
    "    # R = h.parameters\n",
    "    # b = h.parameters\n",
    "    # W, R, b = get_weights(lstm_model, batch_size=len())\n",
    "    # dh_list = np.diff(h_list)\n",
    "    # dW_list = np.diff()\n",
    "\n",
    "    # update theta\n",
    "    theta = updating_theta(lstm, train_loader, h_list, theta, mu, alphas, a_idx)\n",
    "\n",
    "    # update lstm\n",
    "    lstm = update_lstm(lstm, theta)\n",
    "\n",
    "    return lstm\n",
    "\n",
    "\n",
    "### ALGORITHM START ###\n",
    "k = -1\n",
    "while k < 20:  # TODO, (kappa(theta_next, alpha_next) - kappa(theta, alpha) < eps)\n",
    "\n",
    "    # track branch number for tracking what jet_track array to use\n",
    "    k += 1\n",
    "\n",
    "    # W, R, b = get_weights(model=lstm_model, batch_size=batch_size)\n",
    "    # h_bar_list = [[h_bar.detach().numpy()[0] for h_bar in h_bar_list]]\n",
    "    h_bar_list, theta = lstm_results(lstm_model, train_loader)\n",
    "    h_bar_list_np = np.array([h_bar.detach().numpy() for h_bar in h_bar_list])\n",
    "\n",
    "    svm_model.fit(h_bar_list_np)\n",
    "    alphas = np.abs(svm_model.dual_coef_)\n",
    "    a_idx = svm_model.support_\n",
    "\n",
    "    lstm_model = optimization(\n",
    "        lstm_model,\n",
    "        train_loader,\n",
    "        alphas,\n",
    "        a_idx,\n",
    "        learning_rate,\n",
    "        h_list=h_bar_list,\n",
    "        theta=theta,\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    # get rho?\n",
    "    for i in range(h_bar_list):\n",
    "        rho = 0\n",
    "        for j in range(h_bar_list):\n",
    "            rho += alpha[j] * alpha[i] * h_bar_list[j] * h_bar_list[i]\n",
    "    \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
